@article{baconOptionCriticArchitecture2017,
  title = {The {{Option-Critic Architecture}}},
  author = {Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
  year = {2017},
  month = feb,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {31},
  number = {1},
  issn = {2374-3468},
  doi = {10.1609/aaai.v31i1.10916},
  urldate = {2025-05-07},
  abstract = {Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging.We tackle this problem in the framework of options [Sutton,Precup and Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework.},
  copyright = {Copyright (c)},
  langid = {english},
  file = {/home/vaishnavahari/Zotero/storage/DAS5LHKH/Bacon et al. - 2017 - The Option-Critic Architecture.pdf}
}

@misc{earleHierarchicalSubtaskDiscovery2017,
  title = {Hierarchical {{Subtask Discovery With Non-Negative Matrix Factorization}}},
  author = {Earle, Adam C. and Saxe, Andrew M. and Rosman, Benjamin},
  year = {2017},
  month = aug,
  number = {arXiv:1708.00463},
  eprint = {1708.00463},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1708.00463},
  urldate = {2025-05-07},
  abstract = {Hierarchical reinforcement learning methods offer a powerful means of planning flexible behavior in complicated domains. However, learning an appropriate hierarchical decomposition of a domain into subtasks remains a substantial challenge. We present a novel algorithm for subtask discovery, based on the recently introduced multitask linearly-solvable Markov decision process (MLMDP) framework. The MLMDP can perform never-before-seen tasks by representing them as a linear combination of a previously learned basis set of tasks. In this setting, the subtask discovery problem can naturally be posed as finding an optimal low-rank approximation of the set of tasks the agent will face in a domain. We use non-negative matrix factorization to discover this minimal basis set of tasks, and show that the technique learns intuitive decompositions in a variety of domains. Our method has several qualitatively desirable features: it is not limited to learning subtasks with single goal states, instead learning distributed patterns of preferred states; it learns qualitatively different hierarchical decompositions in the same domain depending on the ensemble of tasks the agent will face; and it may be straightforwardly iterated to obtain deeper hierarchical decompositions.},
  archiveprefix = {arXiv},
  file = {/home/vaishnavahari/Zotero/storage/IBHWVXTV/Earle et al. - 2017 - Hierarchical Subtask Discovery With Non-Negative Matrix Factorization.pdf;/home/vaishnavahari/Zotero/storage/47RFS9TU/1708.html}
}

@inproceedings{haarnojaLatentSpacePolicies2018,
  title = {Latent {{Space Policies}} for {{Hierarchical Reinforcement Learning}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Haarnoja, Tuomas and Hartikainen, Kristian and Abbeel, Pieter and Levine, Sergey},
  year = {2018},
  month = jul,
  pages = {1851--1860},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-05-07},
  abstract = {We address the problem of learning hierarchical deep neural network policies for reinforcement learning. In contrast to methods that explicitly restrict or cripple lower layers of a hierarchy to force them to use higher-level modulating signals, each layer in our framework is trained to directly solve the task, but acquires a range of diverse strategies via a maximum entropy reinforcement learning objective. Each layer is also augmented with latent random variables, which are sampled from a prior distribution during the training of that layer. The maximum entropy objective causes these latent variables to be incorporated into the layer's policy, and the higher level layer can directly control the behavior of the lower layer through this latent space. Furthermore, by constraining the mapping from latent variables to actions to be invertible, higher layers retain full expressivity: neither the higher layers nor the lower layers are constrained in their behavior. Our experimental evaluation demonstrates that we can improve on the performance of single-layer policies on standard benchmark tasks simply by adding additional layers, and that our method can solve more complex sparse-reward tasks by learning higher-level policies on top of high-entropy skills optimized for simple low-level objectives.},
  langid = {english},
  file = {/home/vaishnavahari/Zotero/storage/J7L4ND38/Haarnoja et al. - 2018 - Latent Space Policies for Hierarchical Reinforcement Learning.pdf;/home/vaishnavahari/Zotero/storage/MM87QYTH/Haarnoja et al. - 2018 - Latent Space Policies for Hierarchical Reinforcement Learning.pdf}
}

@article{hutsebaut-buysseHierarchicalReinforcementLearning2022,
  title = {Hierarchical {{Reinforcement Learning}}: {{A Survey}} and {{Open Research Challenges}}},
  shorttitle = {Hierarchical {{Reinforcement Learning}}},
  author = {{Hutsebaut-Buysse}, Matthias and Mets, Kevin and Latr{\'e}, Steven},
  year = {2022},
  month = mar,
  journal = {Machine Learning and Knowledge Extraction},
  volume = {4},
  number = {1},
  pages = {172--221},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2504-4990},
  doi = {10.3390/make4010009},
  urldate = {2025-05-07},
  abstract = {Reinforcement learning (RL) allows an agent to solve sequential decision-making problems by interacting with an environment in a trial-and-error fashion. When these environments are very complex, pure random exploration of possible solutions often fails, or is very sample inefficient, requiring an unreasonable amount of interaction with the environment. Hierarchical reinforcement learning (HRL) utilizes forms of temporal- and state-abstractions in order to tackle these challenges, while simultaneously paving the road for behavior reuse and increased interpretability of RL systems. In this survey paper we first introduce a selection of problem-specific approaches, which provided insight in how to utilize often handcrafted abstractions in specific task settings. We then introduce the Options framework, which provides a more generic approach, allowing abstractions to be discovered and learned semi-automatically. Afterwards we introduce the goal-conditional approach, which allows sub-behaviors to be embedded in a continuous space. In order to further advance the development of HRL agents, capable of simultaneously learning abstractions and how to use them, solely from interaction with complex high dimensional environments, we also identify a set of promising research directions.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  file = {/home/vaishnavahari/Zotero/storage/94V8FYEY/Hutsebaut-Buysse et al. - 2022 - Hierarchical Reinforcement Learning A Survey and Open Research Challenges.pdf}
}

@inproceedings{mehrotraExtractingHierarchiesSearch2017,
  title = {Extracting {{Hierarchies}} of {{Search Tasks}} \& {{Subtasks}} via a {{Bayesian Nonparametric Approach}}},
  booktitle = {Proceedings of the 40th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Mehrotra, Rishabh and Yilmaz, Emine},
  year = {2017},
  month = aug,
  series = {{{SIGIR}} '17},
  pages = {285--294},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3077136.3080823},
  urldate = {2025-05-07},
  abstract = {A significant amount of search queries originate from some real world information need or tasks [13]. In order to improve the search experience of the end users, it is important to have accurate representations of tasks. As a result, significant amount of research has been devoted to extracting proper representations of tasks in order to enable search systems to help users complete their tasks, as well as providing the end user with better query suggestions [9], for better recommendations [41], for satisfaction prediction [36] and for improved personalization in terms of tasks [24, 38]. Most existing task extraction methodologies focus on representing tasks as flat structures. However, tasks often tend to have multiple subtasks associated with them and a more naturalistic representation of tasks would be in terms of a hierarchy, where each task can be composed of multiple (sub)tasks. To this end, we propose an efficient Bayesian nonparametric model for extracting hierarchies of such tasks \&amp; subtasks. We evaluate our method based on real world query log data both through quantitative and crowdsourced experiments and highlight the importance of considering task/subtask hierarchies.},
  isbn = {978-1-4503-5022-8},
  file = {/home/vaishnavahari/Zotero/storage/8I9ZSHA3/Mehrotra and Yilmaz - 2017 - Extracting Hierarchies of Search Tasks & Subtasks via a Bayesian Nonparametric Approach.pdf}
}

@article{pateriaHierarchicalReinforcementLearning2021,
  title = {Hierarchical {{Reinforcement Learning}}: {{A Comprehensive Survey}}},
  shorttitle = {Hierarchical {{Reinforcement Learning}}},
  author = {Pateria, Shubham and Subagdja, Budhitama and Tan, Ah-hwee and Quek, Chai},
  year = {2021},
  month = jun,
  journal = {ACM Comput. Surv.},
  volume = {54},
  number = {5},
  pages = {109:1--109:35},
  issn = {0360-0300},
  doi = {10.1145/3453160},
  urldate = {2025-05-07},
  abstract = {Hierarchical Reinforcement Learning (HRL) enables autonomous decomposition of challenging long-horizon decision-making tasks into simpler subtasks. During the past years, the landscape of HRL research has grown profoundly, resulting in copious approaches. A comprehensive overview of this vast landscape is necessary to study HRL in an organized manner. We provide a survey of the diverse HRL approaches concerning the challenges of learning hierarchical policies, subtask discovery, transfer learning, and multi-agent learning using HRL. The survey is presented according to a novel taxonomy of the approaches. Based on the survey, a set of important open problems is proposed to motivate the future research in HRL. Furthermore, we outline a few suitable task domains for evaluating the HRL approaches and a few interesting examples of the practical applications of HRL in the Supplementary Material.},
  file = {/home/vaishnavahari/Zotero/storage/ZSA9KFME/Pateria et al. - 2021 - Hierarchical Reinforcement Learning A Comprehensive Survey.pdf}
}

@misc{qiuIdentifyingSelectionsUnsupervised2024,
  title = {Identifying {{Selections}} for {{Unsupervised Subtask Discovery}}},
  author = {Qiu, Yiwen and Zheng, Yujia and Zhang, Kun},
  year = {2024},
  month = oct,
  number = {arXiv:2410.21616},
  eprint = {2410.21616},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.21616},
  urldate = {2025-05-07},
  abstract = {When solving long-horizon tasks, it is intriguing to decompose the high-level task into subtasks. Decomposing experiences into reusable subtasks can improve data efficiency, accelerate policy generalization, and in general provide promising solutions to multi-task reinforcement learning and imitation learning problems. However, the concept of subtasks is not sufficiently understood and modeled yet, and existing works often overlook the true structure of the data generation process: subtasks are the results of a \${\textbackslash}textit\{selection\}\$ mechanism on actions, rather than possible underlying confounders or intermediates. Specifically, we provide a theory to identify, and experiments to verify the existence of selection variables in such data. These selections serve as subgoals that indicate subtasks and guide policy. In light of this idea, we develop a sequential non-negative matrix factorization (seq- NMF) method to learn these subgoals and extract meaningful behavior patterns as subtasks. Our empirical results on a challenging Kitchen environment demonstrate that the learned subtasks effectively enhance the generalization to new tasks in multi-task imitation learning scenarios. The codes are provided at https://anonymous.4open.science/r/Identifying{\textbackslash}\_Selections{\textbackslash}\_for{\textbackslash}\_Unsupervised{\textbackslash}\_Subtask{\textbackslash}\_Discovery/README.md.},
  archiveprefix = {arXiv},
  file = {/home/vaishnavahari/Zotero/storage/EMC76FF9/Qiu et al. - 2024 - Identifying Selections for Unsupervised Subtask Discovery.pdf;/home/vaishnavahari/Zotero/storage/27MS2I6V/2410.html}
}

@article{suttonMDPsSemiMDPsFramework1999,
  title = {Between {{MDPs}} and Semi-{{MDPs}}: {{A}} Framework for Temporal Abstraction in Reinforcement Learning},
  shorttitle = {Between {{MDPs}} and Semi-{{MDPs}}},
  author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
  year = {1999},
  month = aug,
  journal = {Artificial Intelligence},
  volume = {112},
  number = {1-2},
  pages = {181--211},
  issn = {00043702},
  doi = {10.1016/S0004-3702(99)00052-1},
  urldate = {2025-04-21},
  abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options---closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macroutility problem. ï›™ 1999 Published by Elsevier Science B.V. All rights reserved.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/vaishnavahari/Zotero/storage/M5JC2JBL/Sutton et al. - 1999 - Between MDPs and semi-MDPs A framework for temporal abstraction in reinforcement learning.pdf}
}

@inproceedings{vezhnevetsFeUdalNetworksHierarchical2017,
  title = {{{FeUdal Networks}} for {{Hierarchical Reinforcement Learning}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
  year = {2017},
  month = jul,
  pages = {3540--3549},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-05-07},
  abstract = {We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a slower time scale and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation.},
  langid = {english},
  file = {/home/vaishnavahari/Zotero/storage/6Y8D72T8/Vezhnevets et al. - 2017 - FeUdal Networks for Hierarchical Reinforcement Learning.pdf}
}

@article{wangReinforcementLearningTransfer2014,
  title = {Reinforcement Learning Transfer Based on Subgoal Discovery and Subtask Similarity},
  author = {Wang, Hao and Fan, Shunguo and Song, Jinhua and Gao, Yang and Chen, Xingguo},
  year = {2014},
  month = jul,
  journal = {IEEE/CAA Journal of Automatica Sinica},
  volume = {1},
  number = {3},
  pages = {257--266},
  issn = {2329-9274},
  doi = {10.1109/JAS.2014.7004683},
  urldate = {2025-05-07},
  abstract = {This paper studies the problem of transfer learning in the context of reinforcement learning. We propose a novel transfer learning method that can speed up reinforcement learning with the aid of previously learnt tasks. Before performing extensive learning episodes, our method attempts to analyze the learning task via some exploration in the environment, and then attempts to reuse previous learning experience whenever it is possible and appropriate. In particular, our proposed method consists of four stages: 1) subgoal discovery, 2) option construction, 3) similarity searching, and 4) option reusing. Especially, in order to fulfill the task of identifying similar options, we propose a novel similarity measure between options, which is built upon the intuition that similar options have similar state-action probabilities. We examine our algorithm using extensive experiments, comparing it with existing methods. The results show that our method outperforms conventional non-transfer reinforcement learning algorithms, as well as existing transfer learning methods, by a wide margin.},
  file = {/home/vaishnavahari/Zotero/storage/Y7Q7LBE9/Wang et al. - 2014 - Reinforcement learning transfer based on subgoal discovery and subtask similarity.pdf}
}
