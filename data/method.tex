%  Explain how I am planning to proceed
%  Analyse the problem

\subsection{Latent Space Policy}

In this work \cite{haarnojaLatentSpacePolicies2018} is for when fixed or handcrafted options are available. But the problem formulation is can be extended to our case where the "boundary" of the options is learned.

Summary of \cite{haarnojaLatentSpacePolicies2018}: Represent policy in terms of latent variables arranged in a hierarchy. During training, each layer is trained with a different reward function and is optimized using entropy based method \cite{haarnojaReinforcementLearningDeep}. This also ensures that the relationship between the levels is invertible. During inference, the \textit{h} which represents actions inbetween the leyers flows top-down.

The biggest hurdle when extending this approach is that reward spec function for the options is not available.
The open question is if the VLA model can be used in place of the reward spec function of the options or if the VLA model lacks the expressiveness to do so.

Another approach could be to extend the above method and apply it "horizontally" in addition to the "vertical" approach. This means that the latent space is not only arranged in a hierarchy but also in a grid-like structure.

\subsection{Learn from Expert Policies}

Alternatively, The VLA model for high level plan \cite{patzoldLeveragingVisionLanguageModels2025} and learn options from a basket of expert policies.
% Here, the expert policies 
