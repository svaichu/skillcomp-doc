\subsection{Decomposition of task}
The first challenge is to decompose a task into smaller sub-tasks, which can then be executed by a subset of skills. 
In other words, given a set of skills, how do we select a subset of skills to execute a task during inference? This is a well-studied problem in the field of Hierarchical Reinforcement Learning (HRL) \cite{hutsebaut-buysseHierarchicalReinforcementLearning2022}. 
In HRL, a high-level policy selects a low-level policy from a set of low-level policies, also referred to as \textit{options} \cite{suttonMDPsSemiMDPsFramework1999}. 
In other words, the high-level policy is a policy over options, and the low-level policies are policies over primitive actions. 
There could be multiple levels of such a hierarchy, where middle-level policies are also policies over options, but this is beyond the scope of this work. 
Each \textit{option} (or low-level policy) is temporally extended and has its own termination condition. Policy gradient methods have been extended to this case \cite{baconOptionCriticArchitecture2017}.

\subsection{Subtask discovery}
The next challenge is to identify which skills to learn. 
During training, the high-level policy tries to match the current part of the task the agent is facing with an option available to it. 
However, this leads to problems with defining boundaries between subtasks. This is usually done by:

\begin{itemize}
\item \textbf{Bottom-up approach:} 
Handpicking a set of primitive skills intuitively and training them individually. Later, they are frozen and put together for training the high-level policy. This method is widely used \cite{pateriaHierarchicalReinforcementLearning2021}.
\end{itemize}

However, this approach cannot be generalized. More importantly, handcrafting the set of skills is not optimal, as the set of options defined could be exhaustive and contain human bias in subtask boundary definition \cite{silverWelcomeEraExperience}. This can be overcome by using the following approach:

\begin{itemize}
\item \textbf{Top-down approach:}
Train both high-level and low-level policies together. This introduces the following hurdles: a non-stationary transition function for the high-level policy and effective exploration by options. 
\end{itemize}

\subsection{Non-stationary transition function}
This problem occurs because the state transition for the action (i.e., selecting a low-level policy) taken by the high-level policy is dependent on the low-level policy, which is still learning.

This can be tackled by learning the high-level policy independently of the option's policy gradient. 
The Manager-Worker architecture \cite{vezhnevetsFeUdalNetworksHierarchical2017} addresses this approach.

\subsection{Effective exploration of subtask}
Because the options (or low-level policies) are trained along with the high-level policy, it is difficult to ensure that all the options have explored their subtask space well. 
In other words, for an option to try a new set of actions, it needs to be part of the high-level policy task, which may not always be the case. 
This is accomplished in a data-efficient way by using a suitable model of the environment. 
However, learning such a model from experience is still an open problem \cite{hutsebaut-buysseHierarchicalReinforcementLearning2022}.

\subsection{Distillation}
There has been recent work on utilizing Vision-Language-Action (VLA) models to train expert RL policies \cite{xiangVLAModelExpertCollaboration2025}. 
Additionally, Model-based RL would address the former problem. 
Combining these two approaches, it could be a good idea to utilize a VLA model to act as both a reward model and a teacher, effectively distilling the knowledge of the VLA model into an RL agent for that task.