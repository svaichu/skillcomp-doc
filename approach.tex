\subsection{Decomposition of task}
First challenge is to decompose a task into smaller sub-tasks, which then can be executed by a subset of skills. Or in other words, given a set of skills, how to select a subset of skills to execute a task during inference. This is a well studied problem in the field of Hierarchical Reinforcement Learning (HRL) \cite{hutsebaut-buysseHierarchicalReinforcementLearning2022}. In HRL, a high level policy selects a low level policy from a set of low level policies, this is also called as \textit{options} \cite{suttonMDPsSemiMDPsFramework1999}. In other words, the high level policy is a policy over options and the low level policies are a policy over primitive actions. There could be multiple levels of such hirarchy, where middle level policies are also policy over options, but this is beyond the scope of this work. Each \textit{option} (or a low level policy) is temporally extended and has its own termination condition. Policy gradients methods have been extended to this case \cite{baconOptionCriticArchitecture2017}.

\subsection{Subtask discovery}
Next challenge is to identify which skills to learn during training. This is usually done by,

\begin{itemize}
\item \textbf{Bottom-up approach:} 
Handpicking a set of primitive skills, intuitively, and training them individually. Later, they are frozen and put together for training the high-level policy. This method is widely used \cite{pateriaHierarchicalReinforcementLearning2021}.
\end{itemize}

However, this approach can not be generalized. And also handcrafting the set of skills is not optimal \cite{silverWelcomeEraExperience}. This can be overcome by using the following approach:

\begin{itemize}
\item \textbf{Top-down approach:}
Train both high-level and low-level policies together. This introduces the following hurdles: non stationary transistion function for the high-level policy and effective exploration by options. 
\end{itemize}

\subsection{Non-stationary transition function}
This problem occurs because the state transistion for the action (ie selecting a low-level policy) taken by high-level policy is dependent on the low-level policy, which in itself is still learning.

This can be can be tackled by learning high-level policy independent of option's policy gradient. 
Manager-Worker architecture \cite{vezhnevetsFeUdalNetworksHierarchical2017} adresses this approach.

\subsection{Effective exploration of subtask}
Beacuse the options (or low-level policies) is trained along with the high-level policy, it is difficult to ensure that all the options have explored their subtask space well. Or in other words, for a option to try a new set of actions, it needs to part of the high-level policy task, which may always not be the case.
This is accomplished in data-efficient way by using a suitable model of the environment. But, learning such a model from experience is still an open problem.

\subsection{Distillation}
There have been recent works on utilizing Vision-Language-Action (VLA) models to train expert RL policy \cite{xiangVLAModelExpertCollaboration2025}. Also Model-based RL would address the former problem. 
Combining these two approaches, it could be a good idea to utilize a VLA model to act as both reward model and a teacher, effectively distilling the knowledge of the VLA model into a RL agent for that task. 

% non stationary transition function - feudal 
% subtask disco - model
% training time

% policy gradient methods have been extended to this case \cite{baconOptionCriticArchitecture2017}.